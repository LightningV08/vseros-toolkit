{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684af89e",
   "metadata": {},
   "source": [
    "# 03 • Model & Blending (CV → OOF → Calib/τ → Blend → Test)\n",
    "\n",
    "Эта тетрадка отвечает за обучение и блендинг моделей на подготовленных фичах.\n",
    "Она берёт данные из `artifacts/sets/<run_tag>/…`, тренирует 1–3 кандидата,\n",
    "делает калибровку/порог, блендинг и выдаёт итоговые предсказания.\n",
    "\n",
    "Артефакты моделей кладутся в `artifacts/models/<run_id>`; ничего не скрывается.\n",
    "Все обучаемые трансформы, калибровки и пороги считаются **только по OOF** (anti-leak).\n",
    "\n",
    "Минимальный план: один GBDT + одна линейка. Усиление: добавить бленд + калибровку/τ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, gc, math, warnings, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = os.getenv(\"BASE\", \".\")\n",
    "sys.path.append(str(Path(BASE).resolve()))\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kw): return x\n",
    "\n",
    "# наши модули\n",
    "from common.io import load_set\n",
    "from common.models import gbdt, linear, blend, calibration, thresholds, eval as ME, artifacts as MA\n",
    "from common.features import assemble  # для валидации пар\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def short_hash(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode()).hexdigest()[:8]\n",
    "\n",
    "def mem_gb(obj) -> float:\n",
    "    if hasattr(obj, \"memory_usage\"):\n",
    "        try: return float(obj.memory_usage(deep=True).sum())/(1024**3)\n",
    "        except: pass\n",
    "    try: return float(np.array(obj).nbytes)/(1024**3)\n",
    "    except: return 0.0\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e975e",
   "metadata": {},
   "source": [
    "## Источники фич: из сохранённого набора\n\n* Читаем всё из `artifacts/sets/<run_tag>`: dense/sparse, таргет, фолды, метаданные.\n* Если `FOLDS` на диске нет — fallback ниже построит KFold.\n* Каталог фич подтягиваем из `meta.json` (если нужен для анализа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d09f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— откуда брать фичи\n",
    "RUN_TAG    = \"exp01\"   # набор фичей из artifacts/sets/<RUN_TAG>\n",
    "\n",
    "# ——— базовые столбцы (для метаданных/разрезов)\n",
    "ID_COL     = \"id\"\n",
    "TARGET_COL = None      # заполнить, если есть\n",
    "DATE_COL   = None\n",
    "GEO_COLS   = []        # например [\"lat_bin_1000\"]\n",
    "\n",
    "# ——— тип задачи и метрика\n",
    "TASK_TYPE      = \"binary\"      # regression | binary | multiclass | multilabel\n",
    "PRIMARY_METRIC = \"roc_auc\"     # см. get_scorer в common.models.eval\n",
    "TOP_K          = None          # для multilabel@k\n",
    "\n",
    "# ——— флаги\n",
    "FAST = True   # ускоренные профили кандидатов\n",
    "SAFE = True   # строгие анти-утечки/фиксация сидов\n",
    "\n",
    "# ——— управление кандидатами\n",
    "USE_GBDT_DENSE    = True\n",
    "USE_LINEAR_SPARSE = True\n",
    "USE_HYBRID        = False   # например, отдельный GBDT на поднаборе столбцов (если нужно)\n",
    "\n",
    "# ——— настройка GBDT/Linear по умолчанию (можно менять ниже)\n",
    "GBDT_PARAMS = dict(n_estimators=400 if FAST else 800, learning_rate=0.05, max_depth=8, subsample=0.9)\n",
    "GBDT_LIB    = \"lightgbm\"  # \"lightgbm\"|\"catboost\"|\"xgboost\"\n",
    "LIN_ALGO    = \"lr\"        # \"lr\"|\"sgd\"|\"ridge\"|\"lasso\"\n",
    "LIN_PARAMS  = dict(C=2.0, max_iter=200, n_jobs=8)\n",
    "\n",
    "SEED   = 42\n",
    "N_JOBS = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense_tr, X_dense_te, X_sparse_tr, X_sparse_te, y, FOLDS, meta = load_set(RUN_TAG)\n",
    "\n",
    "feature_catalog = meta.get(\"catalog\", {})\n",
    "ID_COL     = ID_COL or meta.get(\"id_col\", \"id\")\n",
    "TARGET_COL = TARGET_COL or meta.get(\"target_col\", TARGET_COL)\n",
    "\n",
    "if X_dense_tr is not None and X_dense_te is not None:\n",
    "    assemble._validate_pair(X_dense_tr, X_dense_te)\n",
    "if X_sparse_tr is not None and X_sparse_te is not None:\n",
    "    assert X_sparse_tr.shape[1] == X_sparse_te.shape[1], \"sparse dims mismatch\"\n",
    "\n",
    "print(\"Dense:\", None if X_dense_tr is None else X_dense_tr.shape,\n",
    "      \"| Sparse:\", None if X_sparse_tr is None else X_sparse_tr.shape)\n",
    "print(\"Folds:\", len(FOLDS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f70924",
   "metadata": {},
   "source": [
    "## Фолды: что считаем «правильным»\n",
    "\n",
    "* Лучше использовать `FOLDS` из слоя фич (Time/Group/KFold), чтобы согласовать OOF.\n",
    "* Если нет — ниже fallback на `KFold`.\n",
    "* Перед анализом полезно посмотреть размеры валидаций по фолдам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "if not FOLDS:\n",
    "    print(\"[info] FOLDS не переданы — строим KFold fallback\")\n",
    "    idx = np.arange(X_dense_tr.shape[0] if X_dense_tr is not None else X_sparse_tr.shape[0])\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    FOLDS = [(tr, va) for tr, va in kf.split(idx)]\n",
    "print(\"Folds:\", len(FOLDS), \"| fold sizes:\", [len(va) for _,va in FOLDS][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487996d",
   "metadata": {},
   "source": [
    "## Кандидаты: когда какой\n",
    "\n",
    "* **GBDT на DENSE** — табличка/гео/img: быстрый и сильный.\n",
    "* **Линейка на SPARSE** — TF-IDF/char: дешёвый и надёжный.\n",
    "* **Гибрид** — по желанию (поднабор столбцов или иные гиперы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES = []\n",
    "\n",
    "def register_candidate(name: str, run):\n",
    "    CANDIDATES.append({\"name\": name, \"run\": run})\n",
    "    print(f\"[ok] {name}: CV {run.cv_mean:.5f} ± {run.cv_std:.5f} → {run.artifacts_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2304ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GBDT_DENSE and (X_dense_tr is not None):\n",
    "    try:\n",
    "        feat_hash = short_hash(\"dense:\" + \",\".join(map(str, X_dense_tr.columns[:50]))) if hasattr(X_dense_tr, \"columns\") else short_hash(\"dense:\"+str(X_dense_tr.shape[1]))\n",
    "        run_id = MA.make_run_id(task=TASK_TYPE, model=f\"{GBDT_LIB}\", feat_hash=feat_hash, seed=SEED)\n",
    "        run = gbdt.train_cv(\n",
    "            X_dense_tr, y, X_dense_te, FOLDS,\n",
    "            params=GBDT_PARAMS, run_id=run_id, lib=GBDT_LIB,\n",
    "            task=TASK_TYPE, seed=SEED, n_jobs=N_JOBS, save=True, resume=True, show_progress=True, verbose=True\n",
    "        )\n",
    "        register_candidate(\"gbdt_dense\", run)\n",
    "    except Exception as e:\n",
    "        print(\"gbdt_dense error:\", e)\n",
    "else:\n",
    "    print(\"gbdt_dense: пропущен\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LINEAR_SPARSE and (X_sparse_tr is not None):\n",
    "    try:\n",
    "        feat_hash = short_hash(f\"sparse:{X_sparse_tr.shape[1]}\")\n",
    "        run_id = MA.make_run_id(task=TASK_TYPE, model=f\"{LIN_ALGO}\", feat_hash=feat_hash, seed=SEED)\n",
    "        run = linear.train_cv(\n",
    "            X_sparse_tr, y, X_sparse_te, FOLDS,\n",
    "            algo=LIN_ALGO, task=TASK_TYPE, params=LIN_PARAMS,\n",
    "            run_id=run_id, seed=SEED, n_jobs=N_JOBS, save=True, resume=True, show_progress=True, verbose=True\n",
    "        )\n",
    "        register_candidate(\"linear_sparse\", run)\n",
    "    except Exception as e:\n",
    "        print(\"linear_sparse error:\", e)\n",
    "else:\n",
    "    print(\"linear_sparse: пропущен\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b909f9",
   "metadata": {},
   "source": [
    "## (Опционально) Кандидат #3: гибрид\n",
    "\n",
    "Например, обучить ещё один GBDT на поднаборе столбцов или с более консервативными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0deb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HYBRID and (X_dense_tr is not None):\n",
    "    try:\n",
    "        params = {**GBDT_PARAMS, \"max_depth\": 6}\n",
    "        feat_hash = short_hash(\"dense_hybrid:\" + (\",\".join(map(str, X_dense_tr.columns[:50])) if hasattr(X_dense_tr, \"columns\") else str(X_dense_tr.shape[1])))\n",
    "        run_id = MA.make_run_id(task=TASK_TYPE, model=f\"{GBDT_LIB}-hyb\", feat_hash=feat_hash, seed=SEED)\n",
    "        run = gbdt.train_cv(\n",
    "            X_dense_tr, y, X_dense_te, FOLDS,\n",
    "            params=params, run_id=run_id, lib=GBDT_LIB,\n",
    "            task=TASK_TYPE, seed=SEED, n_jobs=N_JOBS, save=True, resume=True, show_progress=True, verbose=True\n",
    "        )\n",
    "        register_candidate(\"gbdt_hybrid\", run)\n",
    "    except Exception as e:\n",
    "        print(\"hybrid error:\", e)\n",
    "else:\n",
    "    print(\"hybrid: пропущен\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc062105",
   "metadata": {},
   "source": [
    "## Сравнение кандидатов: таблица и корреляции\n",
    "\n",
    "Сводка CV (mean/std), ссылки на артефакты и корреляции OOF помогают перед блендингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_oof_matrix(cands):\n",
    "    mats = []\n",
    "    names = []\n",
    "    for c in cands:\n",
    "        r = c[\"run\"]\n",
    "        arr = r.oof_pred\n",
    "        if arr.ndim>1 and arr.shape[1]>1:\n",
    "            arr = arr.max(1)\n",
    "        mats.append(arr.reshape(-1,1))\n",
    "        names.append(c[\"name\"])\n",
    "    return np.hstack(mats), names\n",
    "\n",
    "rows = []\n",
    "for c in CANDIDATES:\n",
    "    r = c[\"run\"]\n",
    "    rows.append(dict(name=c[\"name\"], cv_mean=r.cv_mean, cv_std=r.cv_std, path=str(r.artifacts_path)))\n",
    "cv_df = pd.DataFrame(rows).sort_values(\"cv_mean\", ascending=False) if rows else pd.DataFrame()\n",
    "display(cv_df)\n",
    "\n",
    "try:\n",
    "    if rows:\n",
    "        oof_mat, names = collect_oof_matrix(CANDIDATES)\n",
    "        corr = pd.DataFrame(np.corrcoef(oof_mat.T), index=names, columns=names)\n",
    "        display(corr)\n",
    "except Exception as e:\n",
    "    print(\"corr skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265866f",
   "metadata": {},
   "source": [
    "## Быстрый HPO (микро-рандом)\n",
    "\n",
    "* 10–25 запусков по ключевым гиперпараметрам GBDT (`num_leaves/max_depth/learning_rate`).\n",
    "* Ограничить время на фолд; остановиться, если нет прироста за 10–15 минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26daa8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_hpo_gbdt(n_trials=5):\n",
    "    \"\"\"Пример оболочки для микро-HPO; заполни по необходимости.\"\"\"\n",
    "    trials = []\n",
    "    for i in range(n_trials):\n",
    "        trial_params = {**GBDT_PARAMS}\n",
    "        trial_params[\"max_depth\"] = np.random.choice([6,8,10])\n",
    "        trial_params[\"learning_rate\"] = float(np.random.choice([0.03,0.05,0.1]))\n",
    "        print(f\"[hpo] trial {i+1}/{n_trials}: {trial_params}\")\n",
    "        # вставь запуск train_cv(...) по желанию\n",
    "        trials.append({\"params\": trial_params, \"cv_mean\": None})\n",
    "    return trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083edfc1",
   "metadata": {},
   "source": [
    "## Блендинг: equal-weight / weight-search / level-2\n",
    "\n",
    "* Equal-weight — быстро и часто достаточно.\n",
    "* Weight-search — ищем веса по OOF под целевую метрику.\n",
    "* Level-2 (Ridge/LogReg) — стэкинг с аккуратными OOF-фолдами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ce8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST = None\n",
    "\n",
    "if len(CANDIDATES) >= 2:\n",
    "    try:\n",
    "        run_bl_eq = blend.equal_weight([c[\"run\"] for c in CANDIDATES])\n",
    "        register_candidate(\"blend_eq\", run_bl_eq)\n",
    "    except Exception as e:\n",
    "        print(\"blend_eq error:\", e)\n",
    "\n",
    "    try:\n",
    "        scorer = ME.get_scorer(TASK_TYPE, PRIMARY_METRIC)\n",
    "        ws = blend.weight_search([c[\"run\"] for c in CANDIDATES], y, scorer, nonneg=True, sum_to_one=True)\n",
    "        print(\"weight_search:\", ws)\n",
    "    except Exception as e:\n",
    "        print(\"weight_search error:\", e)\n",
    "\n",
    "    try:\n",
    "        run_bl_l2 = blend.ridge_level2([c[\"run\"] for c in CANDIDATES], y, alpha=1.0)\n",
    "        register_candidate(\"blend_l2\", run_bl_l2)\n",
    "    except Exception as e:\n",
    "        print(\"blend_l2 error:\", e)\n",
    "\n",
    "all_rows = []\n",
    "for c in CANDIDATES:\n",
    "    r = c[\"run\"]\n",
    "    all_rows.append((c[\"name\"], r.cv_mean))\n",
    "all_rows.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"→ лидер:\", all_rows[0] if all_rows else \"—\")\n",
    "BEST = next((c for c in CANDIDATES if c[\"name\"]==all_rows[0][0]), None) if all_rows else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f60e7",
   "metadata": {},
   "source": [
    "## Калибровка и пороги/Top-K\n",
    "\n",
    "* Классификация: Platt/Isotonic по OOF, затем глобальный τ (binary) или per-class/top-K.\n",
    "* Регрессия: калибровка не применяется (оставляем пост-проц в `04_eval_post`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4138f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL = None\n",
    "TAU = None\n",
    "\n",
    "if BEST is not None and TASK_TYPE in (\"binary\",\"multiclass\",\"multilabel\"):\n",
    "    r = BEST[\"run\"]\n",
    "    try:\n",
    "        scorer = ME.get_scorer(TASK_TYPE, PRIMARY_METRIC)\n",
    "        CAL = calibration.fit(r.oof_true, r.oof_pred, method=\"platt\")\n",
    "        oof_cal = calibration.apply(CAL, r.oof_pred)\n",
    "\n",
    "        if TASK_TYPE == \"binary\":\n",
    "            TAU = thresholds.find_global_tau(r.oof_true, oof_cal, scorer)\n",
    "            print(\"τ:\", TAU)\n",
    "        elif TASK_TYPE == \"multilabel\":\n",
    "            if TOP_K is not None:\n",
    "                print(\"Используем top-K =\", TOP_K)\n",
    "            else:\n",
    "                print(\"Можно искать глобальный τ или per-class τ (осторожно)\")\n",
    "    except Exception as e:\n",
    "        print(\"calibration/threshold error:\", e)\n",
    "else:\n",
    "    print(\"Calib skipped\")\n",
    "\n",
    "if CAL is not None and BEST is not None:\n",
    "    run_dir = BEST[\"run\"].artifacts_path\n",
    "    try:\n",
    "        MA.save_array(run_dir, \"calibrator.json\", CAL)\n",
    "    except Exception:\n",
    "        try:\n",
    "            import joblib\n",
    "            joblib.dump(CAL, run_dir/\"calibrator.joblib\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if TAU is not None:\n",
    "        MA.save_array(run_dir, \"thresholds.json\", {\"tau\": float(TAU)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d2680",
   "metadata": {},
   "source": [
    "## Робастность по подгруппам\n",
    "\n",
    "Сделайте разрезы по времени, гео или крупным категориям — ищите провалы метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809df4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_metric(y_true, y_pred, idx, scorer):\n",
    "    return scorer(y_true[idx], y_pred[idx])\n",
    "\n",
    "print(\"Срезы: добавь при необходимости (гео/время/категории)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f661cb",
   "metadata": {},
   "source": [
    "## Финальный инференс на test\n",
    "\n",
    "Можно использовать уже обученные модели (K-fold инференс из `ModelRun.test_pred`) или\n",
    "дотренировать на всём трейне. Ниже — аккуратное применение калибровки/порогов и сохранение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST is not None:\n",
    "    r = BEST[\"run\"]\n",
    "    test_out = r.test_pred\n",
    "\n",
    "    if CAL is not None:\n",
    "        test_out = calibration.apply(CAL, test_out)\n",
    "\n",
    "    post_meta = {}\n",
    "    if TASK_TYPE == \"binary\" and TAU is not None:\n",
    "        yhat = (test_out >= TAU).astype(int)\n",
    "        post_meta[\"tau\"] = float(TAU)\n",
    "    elif TASK_TYPE == \"multilabel\" and TOP_K is not None:\n",
    "        yhat = thresholds.apply_topk(test_out, TOP_K)\n",
    "        post_meta[\"top_k\"] = int(TOP_K)\n",
    "    else:\n",
    "        yhat = test_out\n",
    "\n",
    "    run_dir = r.artifacts_path\n",
    "    MA.save_array(run_dir, \"test_post.json\", post_meta)\n",
    "    try:\n",
    "        np.save(run_dir/\"test_post.npy\", yhat)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"Final test artifact →\", run_dir)\n",
    "else:\n",
    "    print(\"Нет BEST кандидата\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd1061",
   "metadata": {},
   "source": [
    "## Паспорт решения и «что дальше»\n",
    "\n",
    "* Имя лучшего кандидата, CV mean/std, применялись ли бленд/калибровка/τ.\n",
    "* Дальше: открыть `04_eval_post.ipynb` для финального пост-проц и подготовки сабмита."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e670eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST:\n",
    "    r = BEST[\"run\"]\n",
    "    passport = {\n",
    "        \"best_name\": BEST[\"name\"],\n",
    "        \"run_id\": str(r.run_id),\n",
    "        \"cv_mean\": float(r.cv_mean),\n",
    "        \"cv_std\": float(r.cv_std),\n",
    "        \"task\": TASK_TYPE,\n",
    "        \"metric\": PRIMARY_METRIC,\n",
    "        \"used_models\": [c[\"name\"] for c in CANDIDATES],\n",
    "        \"calibration\": CAL is not None,\n",
    "        \"tau\": None if TAU is None else float(TAU),\n",
    "        \"top_k\": TOP_K,\n",
    "    }\n",
    "    print(json.dumps(passport, ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(\"Паспорт недоступен — нет BEST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b1cba",
   "metadata": {},
   "source": [
    "## «Паник-профиль» на последние 40 минут\n",
    "\n",
    "Если дедлайн совсем близко: обучи **один** GBDT (200–300 деревьев) на `X_dense`\n",
    "+ **одну** линейку на TF-IDF; сделай equal-weight бленд, Platt и глобальный τ, выдай test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panic_run():\n",
    "    local_candidates = []\n",
    "    try:\n",
    "        params = {**GBDT_PARAMS, \"n_estimators\": 250, \"max_depth\": 7}\n",
    "        run_id = MA.make_run_id(task=TASK_TYPE, model=f\"{GBDT_LIB}-panic\", feat_hash=\"panic\", seed=SEED)\n",
    "        run_g = gbdt.train_cv(\n",
    "            X_dense_tr, y, X_dense_te, FOLDS,\n",
    "            params=params, run_id=run_id, lib=GBDT_LIB,\n",
    "            task=TASK_TYPE, seed=SEED, n_jobs=N_JOBS, save=True, resume=True, show_progress=True, verbose=True\n",
    "        )\n",
    "        local_candidates.append(run_g)\n",
    "        print(\"panic gbdt CV\", run_g.cv_mean)\n",
    "    except Exception as e:\n",
    "        print(\"panic gbdt error:\", e)\n",
    "\n",
    "    try:\n",
    "        run_id = MA.make_run_id(task=TASK_TYPE, model=f\"{LIN_ALGO}-panic\", feat_hash=\"panic\", seed=SEED)\n",
    "        run_l = linear.train_cv(\n",
    "            X_sparse_tr, y, X_sparse_te, FOLDS,\n",
    "            algo=LIN_ALGO, task=TASK_TYPE, params={**LIN_PARAMS, \"max_iter\": 150},\n",
    "            run_id=run_id, seed=SEED, n_jobs=N_JOBS, save=True, resume=True, show_progress=True, verbose=True\n",
    "        )\n",
    "        local_candidates.append(run_l)\n",
    "        print(\"panic linear CV\", run_l.cv_mean)\n",
    "    except Exception as e:\n",
    "        print(\"panic linear error:\", e)\n",
    "\n",
    "    if len(local_candidates) >= 2:\n",
    "        run_bl = blend.equal_weight(local_candidates)\n",
    "        print(\"panic blend CV\", run_bl.cv_mean)\n",
    "    else:\n",
    "        print(\"panic: not enough candidates\")\n",
    "\n",
    "# вызвать при необходимости\n",
    "# panic_run()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}